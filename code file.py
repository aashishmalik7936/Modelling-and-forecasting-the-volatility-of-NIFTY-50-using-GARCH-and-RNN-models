# -*- coding: utf-8 -*-
"""vix_models.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KQS3lb5-A4KgxhSLZH6sixaHLviiSPma
"""

from google.colab import files
uploaded=files.upload()

!pip install statsmodels --upgrade

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Activation
from keras.callbacks import EarlyStopping
import math
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
import time

import statsmodels.formula.api as smf
from statsmodels.graphics import tsaplots
import statsmodels.tsa.api as smt
import statsmodels.api as sm
import scipy.stats as scs
from statsmodels.tsa.stattools import adfuller
from pandas.plotting import autocorrelation_plot
from statsmodels.graphics.tsaplots import plot_acf
from statsmodels.tsa.arima_model import ARIMA

seed_value= 42
# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# 2. Set the `python` built-in pseudo-random generator at a fixed value
import random
random.seed(seed_value)

# 3. Set the `numpy` pseudo-random generator at a fixed value
import numpy as np
np.random.seed(seed_value)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
import tensorflow as tf
tf.random.set_seed(seed_value)

df=pd.read_csv("vix_data.csv")
df=df.iloc[::-1]
df=df[df['Close']!=0]
df=df.reset_index()
df.drop(['index'], axis=1, inplace=True)
df.head()

log_ret=[]
log_ret.append(0)
for i in range(1, len(df.iloc[:, 0].values)):
  log_ret.append(np.log(df.iloc[i, 4]/df.iloc[i-1, 4]))

ret=[]
ret.append(0)
for i in range(1, len(df.iloc[:, 0].values)):
  ret.append(df.iloc[i, 4]-df.iloc[i-1, 4])

df['log_returns']=log_ret
df['returns']=ret

df['Date_new']=pd.to_datetime(df["Date"])
df["month_year"]=df["Date_new"].dt.strftime('%Y-%m')

df.describe()

legend_properties = {'weight':'bold'}
fig, axes = plt.subplots(figsize=(18,9), dpi=500)
sns.lineplot(x='Date_new', y='log_returns', data=df, Color='blue')
#plt.plot(df['log_returns'])
plt.title("Plot of India VIX Data", fontsize=18, fontweight="bold")
plt.xlabel("Date", fontsize=18, fontweight="bold")
plt.ylabel("India VIX log returns", fontsize=18, fontweight="bold")
plt.xticks(fontsize=16, fontweight="bold")
plt.legend(fontsize=16, prop=legend_properties)
plt.yticks(fontsize=16, fontweight="bold")
#plt.savefig("india_vix_log_ret.jpg")

df['log_returns'].describe()

"""**Jarque Bera test on log returns of NIFTY 50 volatility**"""

scs.jarque_bera(df['log_returns'])

legend_properties = {'weight':'bold'}
fig, axes = plt.subplots(figsize=(18,9), dpi=500)
plot_acf(df['log_returns'], axes, alpha=None)
plt.title("Autocorrelation Plot of India VIX data", fontsize=18, fontweight="bold")
plt.xlabel("Lags", fontsize=18, fontweight="bold")
plt.ylabel("Correlation Value", fontsize=18, fontweight="bold")
plt.xticks(fontsize=16, fontweight="bold")
plt.legend(fontsize=16, prop=legend_properties)
plt.yticks(fontsize=16, fontweight="bold")
#plt.savefig("vix_acf_data_new.jpg")

import statsmodels.api as sm
sm.tsa.acf(df['log_returns'], nlags=12, qstat=True)

"""**Creating the first difference data on log volatility returns**"""

first_diff=[]
for i in range(1, len(log_ret)):
  first_diff.append(df.iloc[i, 5]-df.iloc[i-1, 5])

sm.tsa.acf(first_diff, nlags=11, qstat=True)

"""**Augmented Dickey fuller test on first difference of NIFTY 50 volatility** """

adf=adfuller(first_diff)
adf[1]

"""**finding lag value using aic and loglikelihood values**"""

from statsmodels.tsa.ar_model import AutoReg
#AR(1)
model = AutoReg(first_diff, lags=1).fit()
model.summary()

#AR(2)
model = AutoReg(first_diff, lags=2).fit()
model.summary()

#AR(3)
model = AutoReg(first_diff, lags=3).fit()
model.summary()

#AR(4)
model = AutoReg(first_diff, lags=4).fit()
model.summary()

#AR(5)
model = AutoReg(first_diff, lags=5).fit()
model.summary()

#AR(6)
model = AutoReg(first_diff, lags=6).fit()
model.summary()

"""The lag of 4 values found to be most suitable for making the volatility models

# **Checking the presence of arch effects**
"""

!pip install arch
from arch import arch_model

model = arch_model(first_diff, vol='ARCH', lags=4).fit()

model.summary()

"""# **Checking the residual auto-correlation** """

sm.tsa.acf(model.resid, nlags=11, qstat=True)

"""**ARCH-LM test**"""

import statsmodels
 arch_lm=statsmodels.stats.diagnostic.het_arch(model.resid, nlags=4, store=True)

arch_lm

"""# **Fitting the GARCH model**"""

#Garch(1, 1) with lags=4
model_garch = arch_model(first_diff, mean='AR', vol='Garch', lags=4, p=1, q=1).fit(disp='off')
model_garch.summary()

from statsmodels.stats.stattools import durbin_watson
durbin_watson(model_garch.resid[4:])

"""# **Fitting the EGARCH model**"""

#EGarch(1, 1) with lags=4
model_egarch = arch_model(first_diff, mean='AR', vol='EGARCH', lags=4, p=1, q=1, o=1).fit(disp='off')
model_egarch.summary()

durbin_watson(model_egarch.resid[4:])

"""# **Fitting the TARCH model**"""

#Tarch(1, 1) with lags=4
model_tarch = arch_model(first_diff, vol='GARCH', mean='AR', lags=4, p=1, q=1, o=1, power=1.0).fit()
model_tarch.summary()

durbin_watson(model_tarch.resid[4:])

"""# **Forecasting the volatility of NIFTY50 using GARCH model**"""

#Forecasting using GARCH for next 15 days
results_garch=model_garch.forecast(horizon=15, method='simulation')
firstdiff_fore_garch=results_garch.variance.iloc[2475, :].values

temp=df['log_returns'].iloc[2474]
forecast_log_ret_garch=[]
for i in range(0, 15):
  forecast_log_ret_garch.append(temp+firstdiff_fore_garch[i])
  temp+=firstdiff_fore_garch[i]

forecast_garch=[]
temp=df['Close'].iloc[2474]
for i in range(0, 15):
  forecast_garch.append(np.exp(forecast_log_ret_garch[i])*temp)
  temp*=np.exp(forecast_log_ret_garch[i])

#actual_values=[21.22, 20.85, 20.25, 20.31, 19.79, 23.00, 20.46, 20.89, 20.40, 22.49, 22.43, 23.03, 22.69, 23.5, 23.08]

forecast_garch

!pip install --upgrade scikit-learn==0.24.2

"""**Mean absolute percentage error on 5 days forecasting data**"""

#mean absolute percentage error 5 days
from sklearn.metrics import mean_absolute_percentage_error
y_true=[21.22, 20.85, 20.25, 20.31, 19.79, 23.00, 20.46, 20.89, 20.40, 22.49, 22.43, 23.03, 22.69, 23.5, 23.08]
mean_absolute_percentage_error(y_true[:5], forecast_garch[:5])

"""**Mean absolute percentage error on 10 days forecasting data**"""

#10 days error
mean_absolute_percentage_error(y_true[:10], forecast_garch[:10])

"""**Mean absolute percentage error on 15 days forecasting data**"""

#15 days error
mean_absolute_percentage_error(y_true[:], forecast_garch[:])

"""# **Forecasting the volatility of NIFTY50 volatility using EGARCH model**


"""

#Forecasting using EGARCH for next 15 days
results_egarch=model_egarch.forecast(horizon=15, method='simulation')
firstdiff_fore_egarch=results_egarch.variance.iloc[2475, :].values

temp=df['log_returns'].iloc[2474]
forecast_log_ret_egarch=[]
for i in range(0, 15):
  forecast_log_ret_egarch.append(temp+firstdiff_fore_egarch[i])
  temp+=firstdiff_fore_egarch[i]

forecast_egarch=[]
temp=df['Close'].iloc[2474]
for i in range(0, 15):
  forecast_egarch.append(np.exp(forecast_log_ret_egarch[i])*temp)
  temp*=np.exp(forecast_log_ret_egarch[i])

forecast_egarch

"""**Mean absolute percentage error on 5 days forecasting data**"""

#5 days error
mean_absolute_percentage_error(y_true[:5], forecast_egarch[:5])

"""**Mean absolute percentage error on 10 days forecasting data**"""

#10 days error
mean_absolute_percentage_error(y_true[:10], forecast_egarch[:10])

"""**Mean absolute percentage error on 15 days forecasting data**"""

#15 days error
mean_absolute_percentage_error(y_true[:], forecast_egarch[:])

"""# **Forecasting the volatility of NIFTY50 volatility using TARCH model**"""

#Forecasting using TARCH for next 15 days
results_tarch=model_tarch.forecast(horizon=15, method='simulation')
firstdiff_fore_tarch=results_tarch.variance.iloc[2475, :].values

temp=df['log_returns'].iloc[2474]
forecast_log_ret_tarch=[]
for i in range(0, 15):
  forecast_log_ret_tarch.append(temp+firstdiff_fore_tarch[i])
  temp+=firstdiff_fore_tarch[i]

forecast_tarch=[]
temp=df['Close'].iloc[2474]
for i in range(0, 15):
  forecast_tarch.append(np.exp(forecast_log_ret_tarch[i])*temp)
  temp*=np.exp(forecast_log_ret_tarch[i])

forecast_tarch

"""**Mean absolute percentage error on 5 days forecasting data**"""

#5 days error
mean_absolute_percentage_error(y_true[:5], forecast_tarch[:5])

"""**Mean absolute percentage error on 10 days forecasting data**"""

#10 days error
mean_absolute_percentage_error(y_true[:10], forecast_tarch[:10])

"""**Mean absolute percentage error on 15 days forecasting data**"""

#15 days error
mean_absolute_percentage_error(y_true[:], forecast_tarch[:])

"""# **Comparing the performance of GARCH, EGRCH & TARCH models using AIC & loglikelihood score**"""

print(model_garch.aic)
print(model_egarch.aic)
print(model_tarch.aic)

print(model_garch.loglikelihood)
print(model_egarch.loglikelihood)
print(model_tarch.loglikelihood)

"""# **Training the LSTM model**"""

seed_value= 42
# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value
import os
os.environ['PYTHONHASHSEED']=str(seed_value)

# 2. Set the `python` built-in pseudo-random generator at a fixed value
import random
random.seed(seed_value)

# 3. Set the `numpy` pseudo-random generator at a fixed value
import numpy as np
np.random.seed(seed_value)

# 4. Set the `tensorflow` pseudo-random generator at a fixed value
import tensorflow as tf
tf.random.set_seed(seed_value)

rnn_df=pd.DataFrame()
rnn_df['lag_4']=first_diff[:2472]
rnn_df['lag_3']=first_diff[1:2473]
rnn_df['lag_2']=first_diff[2:2474]
rnn_df['lag_1']=first_diff[3:2475]
rnn_df['y']=first_diff[4:2476]

rnn_df.head()

from math import log

def calculate_aic(n, mse, num_params):
	aic = n * log(mse) + 2 * num_params
	return aic

def calculate_bic(n, mse, num_params):
	bic = n * log(mse) + num_params * log(n)
	return bic

X=rnn_df.iloc[:, :4].values
y=rnn_df['y'].values

##splitting dataset into train and test split
train_size=int(len(X)*0.75)
train_X,valid_X=X[0:train_size,:],X[train_size:,:]

Trn_X =train_X.reshape(train_X.shape[0],train_X.shape[1] , 1)
val_X =valid_X.reshape(valid_X.shape[0],valid_X.shape[1] , 1)

Trn_X.shape

val_X.shape

Trn_y=y[:1854]
val_y=y[1854:]

model=Sequential()
model.add(LSTM(80, return_sequences=True, input_shape=(4,1)))
model.add(LSTM(80, return_sequences=True))
model.add(LSTM(40))
model.add(Activation('relu'))
model.add(Dense(1))
model.compile(loss='mean_squared_error',optimizer='adam')
# simple early stopping
es = EarlyStopping(monitor='val_loss', patience=1, mode='auto', verbose=1, restore_best_weights=True)

begin = time.time()
history = model.fit(Trn_X,Trn_y,validation_data=(val_X,val_y), epochs=1000, verbose=1, callbacks=[es])
time.sleep(1)
# store end time
end = time.time()

X_trans=X.reshape(X.shape[0],X.shape[1] , 1)
X_pred=model.predict(X_trans)

"""**AIC on training data**"""

#AIC train
calculate_aic(1854, mean_squared_error(y[:1854],X_pred[:1854]), 4)

"""**AIC on validation data**"""

#AIC val
calculate_aic(618, mean_squared_error(y[1854:],X_pred[1854:]), 4)

"""**BIC on training data**"""

#BIC train
calculate_bic(1854, mean_squared_error(y[:1854],X_pred[:1854]), 4)

"""**BIC on validation data**"""

#BIC val
calculate_bic(618, mean_squared_error(y[1854:],X_pred[1854:]), 4)

X.shape

"""# **Forecasting the volatility of NIFTY50 volatility using LSTM model**"""

#Forecasting using LSTM
lag_4=X[2471, 1]; lag_3=X[2471, 2]; lag_2=X[2471, 3]; lag_1=y[2471];
firstdiff_fore_lstm=[];
for i in range(0, 15):
  x_pred=[]; x_pred.append(lag_4); x_pred.append(lag_3); x_pred.append(lag_2); x_pred.append(lag_1);
  X_pred=np.array(x_pred);
  X_pred=X_pred.reshape(1, 4);
  X_final=X_pred.reshape(1, 4, 1);
  pred=model.predict(X_final);
  firstdiff_fore_lstm.append(pred[0][0]);
  lag_4=lag_3;
  lag_3=lag_2;
  lag_2=lag_1;
  lag_1=pred[0][0];

temp=df['log_returns'].iloc[2474]
forecast_log_ret_lstm=[]
for i in range(0, 15):
  forecast_log_ret_lstm.append(temp+firstdiff_fore_lstm[i])
  temp+=firstdiff_fore_lstm[i]

forecast_lstm=[]
temp=df['Close'].iloc[2474]
for i in range(0, 15):
  forecast_lstm.append(np.exp(forecast_log_ret_lstm[i])*temp)
  temp*=np.exp(forecast_log_ret_lstm[i])

forecast_lstm

"""**Mean absolute percentage error on 5 days forecasting data**"""

#5 days error
from sklearn.metrics import mean_absolute_percentage_error
mean_absolute_percentage_error(y_true[:5], forecast_lstm[:5])

"""**Mean absolute percentage error on 10 days forecasting data**"""

#10 days error
from sklearn.metrics import mean_absolute_percentage_error
mean_absolute_percentage_error(y_true[:10], forecast_lstm[:10])

"""**Mean absolute percentage error on 15 days forecasting data**"""

#15 days error
from sklearn.metrics import mean_absolute_percentage_error
mean_absolute_percentage_error(y_true[:], forecast_lstm[:])

